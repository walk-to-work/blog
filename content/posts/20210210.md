+++
title = "行優先(Row-major)でcuBLASのGemmを使う"
tags = ["CUDA"]
date = "2021-02-10"
+++


cuBLASは基本的に列優先(Col-major)であり，下図左の行列は右のように格納されることが前提とされている．

![cuBLASの行列](/20210210/cuBLAS_mat.jpg)

これはcuBLASの元ネタ（？）である，BLASがFortran用の数値計算ライブラリであり，Fortranでの2次元配列はCol-majorだからかな？とは思うがそこまで興味はない．
僕はCやJavaやJava Script等を書いてきたため，どちらかと言えばRow-majorの方が馴染み深く，CUDAを書くときもRow-majorで書いた．
ただ，行列積を求めたいと思いにバチバチに最適化されたライブラリであるcuBLASを使いたいが，メモリアクセスパターンが違うので計算できないという問題に当たったので，
この記事を書いている．

結果として[stack overflow - cublasSgemm row-major multiplication]( https://stackoverflow.com/questions/56043539/cublassgemm-row-major-multiplication)の記事を参考に問題を解決した．

行列Aをcow-majorで配置した配列があるとき，A^Tをcol-majorで配置したものと行列Aをrow-majorで配置したときのメモリ配置同じになる．このこととcuBLASの行列積計算関数cuBLASXGemmは入力を転置することができるという性質を活かすと，$A^TB^T=C$で計算できる．
が，出力配列もcol-majorで配置されるため更に工夫が必要で，$B^TA^T=C^T$で，row-majorのA,Bからrow-majorのCが得られる．

以下は検証コード．

```cuda
#include<stdio.h>
#include<stdlib.h>
#include<math.h>
#include<cublas_v2.h>

template<typename T>
void print_hst_matrix( const char *name , const T *mat , const int rowSize , const int colSize ){
  printf("%s\n", name);
  for( int i = 0 ; i < rowSize ; i++ ){
    for( int j = 0 ; j < colSize ; j++ ){
      printf("%.3f\t", ( mat[ i * colSize + j ] ));
    }
    printf("\n");
  }
  printf("\n");
}

template<typename T  >
__global__ void gemm_TN( const T *MatrixA , const T *MatrixB , T *TargetMatrix , const int ColSizeOfA , const int ColSizeOfB , const int MutualEdgeSize ){
  int row = blockIdx.y*blockDim.y+threadIdx.y;
  int col = blockIdx.x*blockDim.x+threadIdx.x;

  if( row < ColSizeOfA && col < ColSizeOfB ){
    T x = 0.0f;
    for( int mi = 0 ; mi < MutualEdgeSize ; mi++ ){
      x += MatrixA[ row * MutualEdgeSize + mi ] * MatrixB[ col * MutualEdgeSize +  mi ];
    }
    TargetMatrix[ col * ColSizeOfA + row  ] = x;
  }
}


int main(){
  float *MatA , *MatB , *MatC;
  float _a = 1.0 , _b = 0.0;
  unsigned int RowSizeOfA = 3 , ColSizeOfB = 4 , MutualEdgeSize = 5;

  MatA = (float *)malloc( sizeof(float) * RowSizeOfA * MutualEdgeSize );
  MatB = (float *)malloc( sizeof(float) * MutualEdgeSize * ColSizeOfB );
  MatC = (float *)malloc( sizeof(float) * RowSizeOfA * ColSizeOfB );


  int cnt = 0;
  for( int ri = 0 ; ri < RowSizeOfA ; ri++ ){ 
    for( int mi = 0 ; mi < MutualEdgeSize; mi++ ){ 
      MatA[ ri * MutualEdgeSize + mi ] = cnt++;
    }
  }

  cnt = 0;
  for( int mi = 0 ; mi < MutualEdgeSize; mi++ ){  
    for( int ci = 0 ; ci < ColSizeOfB ; ci++ ){ 
      MatB[ mi * ColSizeOfB + ci ] = (cnt++) * 2 ;
    }
  }

  for(int i = 0 ; i < RowSizeOfA * ColSizeOfB ; i++ ) MatC[i] = 0.0f;

  for( int ri = 0 ; ri < RowSizeOfA ; ri++ ){
    for( int ci = 0 ; ci < ColSizeOfB ; ci++ ){
      for(int mi = 0 ; mi < MutualEdgeSize ; mi++ )
        MatC[ ri * ColSizeOfB + ci ] += MatA[ ri * MutualEdgeSize + mi ] * MatB[ mi * ColSizeOfB + ci ];
    }
  }

  print_hst_matrix( "hst a" , MatA , RowSizeOfA , MutualEdgeSize );
  print_hst_matrix( "hst b" , MatB , MutualEdgeSize , ColSizeOfB );
  print_hst_matrix( "computed on hst" , MatC , RowSizeOfA , ColSizeOfB );

  /*
     computed on hst
     240.000 260.000 280.000 300.000
     640.000 710.000 780.000 850.000
     1040.000        1160.000        1280.000        1400.000
   */

  float *dev_MatA , *dev_MatB , *dev_MatC;

  cudaMalloc( (void**)&dev_MatA , sizeof(float) * RowSizeOfA * MutualEdgeSize );
  cudaMalloc( (void**)&dev_MatB , sizeof(float) * MutualEdgeSize * ColSizeOfB );
  cudaMalloc( (void**)&dev_MatC , sizeof(float) * RowSizeOfA * ColSizeOfB     );

  cudaMemcpy( dev_MatA , MatA , sizeof(float) *  RowSizeOfA * MutualEdgeSize , cudaMemcpyHostToDevice );
  cudaMemcpy( dev_MatB , MatB , sizeof(float) *  MutualEdgeSize * ColSizeOfB , cudaMemcpyHostToDevice );

  cublasHandle_t handle;
  cublasCreate(&handle);

  cublasSgemm(
      handle, CUBLAS_OP_N , CUBLAS_OP_N ,
      ColSizeOfB , RowSizeOfA , MutualEdgeSize ,
      &_a,
      dev_MatB , ColSizeOfB ,
      dev_MatA , MutualEdgeSize ,
      &_b,
      dev_MatC , ColSizeOfB );

  cudaDeviceSynchronize();

  cudaMemcpy( MatC , dev_MatC , sizeof(float) * RowSizeOfA * ColSizeOfB , cudaMemcpyDeviceToHost );

  print_hst_matrix( "computed by dev" , MatC , RowSizeOfA , ColSizeOfB );
  /*
     computed by dev
     240.000 260.000 280.000 300.000
     640.000 710.000 780.000 850.000
     1040.000        1160.000        1280.000        1400.000
   */


  free(MatA);
  free(MatB);
  free(MatC);
  cublasDestroy (handle);
  cudaFree( dev_MatA );
  cudaFree( dev_MatB );
  cudaFree( dev_MatC );

}
```
